{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_SV.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyNQBqxVbZ0qrW0kt2B8wU5u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"HDielvLwzNWL","colab_type":"code","colab":{}},"source":["%pwd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EYTtLItkqzgn","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('../content/drive', force_remount=True)  # 내가 경로 설정하는 듯"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oRVwoqdZkTm5","colab_type":"code","colab":{}},"source":["cd ../content/drive/My Drive/30words_classification/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JteuXkClyZ1u","colab_type":"code","colab":{}},"source":["%tensorflow_version 1.x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ai1501_ptcD0","colab_type":"code","colab":{}},"source":["import os\n","#from wav_tools_SV import *\n","import numpy as np\n","import tensorflow as tf\n","\n","from sklearn.preprocessing import MinMaxScaler\n","import matplotlib.pyplot as plt\n","import librosa, librosa.display\n","from sklearn.model_selection import train_test_split\n","\n","def minmax_Batch_size(X):\n","    minmax_X = []\n","    #print(X.shape.as_list())\n","    #exit()\n","    for i in range(len(X)):\n","        temp = X[i]\n","        scaler = MinMaxScaler()\n","        temp = scaler.fit_transform(temp)\n","        minmax_X.append(temp)\n","\n","    return minmax_X\n","\n","def xavier_init(n_inputs, n_outputs, uniform=True):\n","    if uniform:\n","        init_range = tf.sqrt(6.0 / (n_inputs + n_outputs))\n","        return tf.random_uniform_initializer(-init_range, init_range)\n","\n","    else:\n","        stddev = tf.sqrt(3.0 / (n_inputs + n_outputs))\n","        return tf.truncated_normal_initializer(stddev=stddev)\n","\n","############ 2. Feature Extraction ############\n","def plot_mfcc(mfcc, RECORD_FILE_NAME=None):\n","    plt.figure()\n","    librosa.display.specshow(mfcc, x_axis='time')  # y 축은 모르겠당 y_coords=13\n","    # plt.plot(mfcc[1], mfcc[0])\n","    plt.title('MFCC')\n","    plt.xlabel('Time [s]')\n","    plt.ylabel('MFCC Coefficeints')\n","    plt.colorbar()\n","    plt.show()\n","\n","def match_1s(data):\n","    # 30sec 길이 통일\n","    num = 22050\n","    # print(data.shape, end=' ->')\n","    if len(data) < 22050:\n","        num = 22050 - len(data)\n","        temp = np.zeros(num) #* 1e-05\n","        data = np.append(data, temp)\n","    elif len(data) > 22050:\n","        data = data[:22050]\n","\n","    #print(data.shape, end=' ')\n","\n","    # (22050,) to column vector : (2250, 1)\n","    # data = data.reshape(len(data), 1)\n","\n","    return data\n","\n","\n","def feature_mfcc(RECORD_FILE_NAME):\n","\n","    # 조정할 수 있는 건 다 적어보자.\n","\n","    # sr = 22050 = bitrate/2 -> Q. bitrate 와 어떤 관계?\n","    # Generate mfccs from a time series\n","    # t초당 sig.shape = (t*sr,)\n","    sig, sr = librosa.load(RECORD_FILE_NAME)  # , sr=sr\n","    # 만약, sr=16000, mfcc.shape = (n_mfcc,1251)\n","    #       sr=(default)22050, mfcc.shape = (n_mfcc, 1723)\n","\n","\n","    hop_length = 0\n","    if len(sig) == 22050:  # 128 -> mfcc Tx 301, 223 -> mfcc Tx 173\n","        hop_length = 128\n","    elif len(sig) == 38433:\n","        hop_length = 223  # Tx 173 으로 통일\n","    elif len(sig) < 22050:\n","        # print(\"smaller than\", end=' ')\n","        sig = match_1s(sig)\n","        hop_length = 128\n","\n","    else:\n","        # print(len(sig))\n","        sig = match_1s(sig)\n","        print(\"1s over\")\n","        hop_length = 128\n","\n","    n_mfcc = 24\n","    # n_mels = 20\n","    n_fft = 101\n","    fmin = 0\n","    fmax = None\n","    # sr = 16000\n","\n","\n","    mfcc = librosa.feature.mfcc(y=sig, sr=sr, hop_length=hop_length, fmin=fmin, fmax = fmax,\n","                                  n_fft= n_fft, n_mfcc=n_mfcc)\n","    # print(\"here\", mfcc.shape)\n","\n","    return mfcc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FNP2g-DYnVGD","colab_type":"code","colab":{}},"source":["X_train, X_test, Y_train, Y_test = np.load(\"./data.npy\", allow_pickle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D8c1PiZ6nemm","colab_type":"code","colab":{}},"source":["device_name = tf.test.gpu_device_name()\n","\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vL5TjnTlv-UG","colab_type":"code","colab":{}},"source":["# model\n","with tf.device('/gpu:0'):\n","    num_mfcc = 24\n","    num_class = 30\n","    Tx = 173\n","\n","    X = tf.placeholder(tf.float32, [None, Tx, num_mfcc], name=\"X\")\n","    Y = tf.placeholder(tf.float32, [None, num_class], name=\"Y\")\n","\n","\n","    hidden_size = 32\n","    n_layers = 2\n","    learning_rate = 0.001\n","    keep_prob = tf.placeholder(tf.float32, name=\"keep_prob\")\n","\n","\n","\n","    layers = [tf.nn.rnn_cell.GRUCell(num_units=hidden_size,activation='relu')\n","                            for layer in range(n_layers)]\n","    layers_drop = [tf.nn.rnn_cell.DropoutWrapper(\n","                        layer, state_keep_prob= keep_prob)\n","                        for layer in layers]\n","\n","    multi_layer_cell = tf.nn.rnn_cell.MultiRNNCell(layers_drop)\n","    outputs, _state = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)\n","\n","    outputs = tf.contrib.layers.fully_connected(inputs=outputs, num_outputs=1  # 1로 줄이고..\n","                                                , activation_fn=None)  # tf.nn.relu 얘가 default 라니..\n","\n","    # length of sequence to\n","    outputs_ = tf.reshape(outputs, shape=[-1, 1, Tx])\n","    W = tf.get_variable(name=\"last_W\", shape=[1, Tx, num_class],\n","                        initializer= xavier_init(Tx, num_class))\n","    predict = tf.nn.conv1d(outputs_, filters=W, padding=\"VALID\",)  # 1D 쉽다\n","    logits = tf.reshape(predict, shape=[-1, num_class], name=\"predict\")\n","\n","    hypothesis = tf.nn.softmax(logits, name=\"hypothesis\")\n","    cost = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n","    cost = tf.reduce_mean(cost, name='loss')\n","\n","    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost, name=\"MyOpt\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1IXO6yPwxFcH","colab_type":"code","colab":{}},"source":["## traning\n","\n","learning_rate = 0.001\n","training_epochs = 100\n","batch_size = 5\n","total_data = len(X_train)\n","test_acc = []\n","\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    saver = tf.train.Saver() # (max_to_keep=300)\n","    train_writer = tf.summary.FileWriter(\"./log/\", sess.graph)\n","\n","    # saver = tf.train.import_meta_graph('./models/GRU_2_layer/epoch19/cost_1.236314_data_num-26900.meta')\n","    # saver.restore(sess, tf.train.latest_checkpoint('./models/GRU_2_layer/epoch9/'))\n","    # graph = tf.get_default_graph()\n","\n","    # X = graph.get_tensor_by_name(\"X:0\")\n","    # Y = graph.get_tensor_by_name(\"Y:0\")\n","    # cost = graph.get_tensor_by_name(\"loss:0\")\n","    # cost = tf.reduce_mean(cost)\n","    # optimizer = graph.get_operation_by_name(\"MyOpt\")\n","    # keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n","    # hypothesis = graph.get_tensor_by_name(\"hypothesis:0\")\n","\n","\n","    for epoch in range(training_epochs):\n","        # min_cost = 1000\n","        epoch_path = './models/GRU_2_layer/epoch'+ str(epoch)\n","        os.makedirs(epoch_path, exist_ok = True)\n","\n","        print(\"============ epoch\", epoch, \"============\")\n","        for batch in range(0, total_data, batch_size):\n","\n","            batch_xs = X_train[batch:batch + batch_size]\n","            batch_ys = Y_train[batch:batch + batch_size]\n","            minmax_batch_xs = minmax_Batch_size(batch_xs)\n","\n","            feed_dict = {X: minmax_batch_xs , Y: batch_ys, keep_prob: 0.7}\n","            # batch 5개 cost_ 주의.\n","            cost_, _= sess.run([cost, optimizer], feed_dict=feed_dict)\n","\n","            #train_writer.add_summary(summary, global_step= batch)\n","            if (batch % 100 ==0):\n","                print('epoch:', epoch,'data: %04d' % (batch), 'cost = %.9f'%(cost_))\n","                check_path = epoch_path + \"/cost_%.6f_data_num\"% (cost_)\n","                saver.save(sess, check_path, batch)\n","\n","        print('Learning Finished!')\n","\n","        minmax_X_test = minmax_Batch_size(X_test)\n","\n","        minmax_X_test = minmax_X_test[0:3000]\n","        Y_test = Y_test[0:3000]\n","\n","        print(\"label idx: \", end='')\n","        print(np.argmax(Y_test, 1))\n","        print(\"predict idx: \", end='')\n","        print(sess.run(tf.argmax(hypothesis, 1), feed_dict={X: minmax_X_test, keep_prob: 1}))\n","\n","        ## 여러개일 때 유용\n","        correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n","        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","        acc = sess.run(accuracy, feed_dict={X: minmax_X_test, Y: Y_test, keep_prob: 1})\n","        print(\"Accuracy: \", acc)\n","        test_acc.append(acc)    \n","        # np.save('./test_acc.npy', test_acc)\n","        print(\"test accurcy list\")\n","        print(test_acc)\n","        \n","        if epoch > 5:\n","            if accuracy > test_acc[epoch-1] and test_acc[epoch-1] > test_acc[epoch-2] and test_acc[epoch-2] > test_acc[epoch-3]:\n","                print(\"Overfitting at epoch = \", epoch)\n","                exit()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8G62nfWN0GF-","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}